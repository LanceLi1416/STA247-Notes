\chapter{Bivariate Distributions}

\section{Multivariate and Joint Distributions}

Thus far in the course, we have been studying \bred{univariate distributions} -- i.e. how we can model the behaviour or outcomes of a single variable. This is not always realistic or practical as we may sometimes be interested in how two (or usually more) variables occur together.

Consider the studio of colourblindness among males and females:

\begin{center}
    \begin{tabular}{|| c | c c | c ||}
        \hline
                             & Men (M) & Women (M') & Total   \\
        \hline
        \hline
        Colourblind (C)      & $0.04$  & $0.002$    & $0.042$ \\
        \hline
        Not Colourblind (C') & $0.47$  & $0.488$    & $0.958$ \\
        \hline \hline
        Total                & $0.51$  & $0.49$     & $1.00$  \\
        \hline
    \end{tabular}
\end{center}

The table is an example of a \term{joint probability distribution}. 

\begin{definition}[Joint Probability Distributions]\index{Joint Probability Distributions}
    Given two \bred{discrete} random variables $X$ and $Y$, the joint PMF is defined as $$f(x, y) = P(X = x, Y = y) = P(X = x \cap Y = y)$$ where $x$, $y$ are used to denote particular values in the support of $X$ and $Y$. 

    {~~~}

    Given two \bred{continuous} random variables $X$ and $Y$, we have the joint probability function $f(x, y)$ to describe the probability density over the region of $(X, Y)$ outcomes. Recall that \itblue{probability density $\neq$ probability mass}!

    {~~~}

    Any function satisfying the below two properties is a valid probability mass / density function.

    \begin{minipage}[t]{0.45\linewidth}
        \begin{center} Discrete \end{center}
        \begin{enumerate}
            \item $0 \le f(x, y) \le 1$
            \item $\sum_{x \in X} \sum_{y \in Y} P(X = x, Y = y) = 1$
        \end{enumerate}
    \end{minipage}
    \begin{minipage}[t]{0.45\linewidth}
        \begin{center} Continuous \end{center}
        \begin{enumerate}
            \item $f(x, y) \ge 0$ \footnote{The density can exceed $1$, as long as the area $\le 1$}
            \item $\int_{x \in X} \int_{y \in Y} f(x, y) \,dy \,dx = 1$
        \end{enumerate}
    \end{minipage}
\end{definition}

\begin{definition}[Marginal Distributions]\index{Marginal Distributions}
    The \term{marginal probability mass/density functions} can be extracted from the joint distribution, which returns the probability mass/density of one variable only

    \begin{itemize}
        \item $f_X(x) = P(X = x) = \sum_{y \in Y} P(X = x, Y = y)$

              $f_X(x) = P(X = x) = \int_{y \in Y} P(X = x, Y = y) \,dy$

        \item $f_Y(y) = P(Y = y) = \sum_{x \in X} P(X = x, Y = y)$

              $f_Y(y) = P(Y = y) = \int_{x \in X} P(X = x, Y = y) \,dx$
    \end{itemize}

    If $\forall (x, y) \in (X, Y)$, $f(x, y) = f_X(x) \cdot f_Y(y)$, the $X$ and $Y$ are \term{independent}. otherwise, $X$ and $Y$ are said to be \term{dependent}. 
\end{definition}