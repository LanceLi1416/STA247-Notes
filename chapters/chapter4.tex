\chapter{Discrete Distributions}

\section{Random Variable}

\begin{definition}[Random Variable]\index{Random Variable}
    A \term{random variable} is a real-valued function that assigns a numerical value to each event in the sample space $\Omega$ arising from a random experiment. A random variable $X$ is a \bred{real-valued function} $X : \Omega \to \mathbb{R}$ such that for every $\omega \in \Omega$, $X(\omega) = x \in \mathbb{R}$. It is a mapping from the sample space to the real numbers.
\end{definition}

\begin{example}
    Consider the random experiment of tossing a coin. 

    \begin{itemize}
        \item $\Omega = \{ H, T \}$
        \item Let $X$ be the RV denoting the outcome of a toss. We ca define $X$ such that $X(H) = 1$, $X(T) = 0$ essentially converting each outcome into a number. 
        \item By convention, we will denote random variables with capital letters, and a particular (unknown value) of a random variable with its lower case equivalent. i.e. for a random variable $X$, a particular value of this RV would be denoted by $x$.
    \end{itemize}
\end{example}

\begin{definition}[Discrete Random Variable]\index{Discrete Random Variable}\index{Continuous Random Variable}
    A \term{discrete} of a random variable $X$ is one tha can take on only a finite number of a countably infinite number of possible values $x$. A random variable $X$ is \term{continuous} if its domain is an interval of real numbers. 
\end{definition}

\begin{definition}[Probability Mass Function]\index{Probability Mass Function}
    A \term{probability mass function} (PMF) of a discrete random variable is one that assigns a probability to each value $x \in C$ such that 
    \begin{itemize}
        \item $0 \le P(X = x) \le 1$
        \item $\sum_{x \in X} P(X = x) = 1$
    \end{itemize}
\end{definition}

\begin{example} 
    Below are some examples of random variables. 

    \textbf{Discrete RV Examples}
    
    \begin{itemize}
        \item The number of defects in a day's production of car parts 
        \item The number of new arrivals in a queue 
        \item The status of your internet service: online or offline 
        \item The number of students online at a particular time
    \end{itemize}

    \textbf{Continuous RV Examples}

    \begin{itemize}
        \item The weight of a randomly selected individual 
        \item The time it takes to load a video 
        \item The temperature in the morning of a random day
    \end{itemize}
\end{example}

\begin{example}
    Determine the value of $k$ such that $f(x) = \frac{kx^2 - x + 2}{4}$ will be a valid probability mass function for $X = \{ 0, 1, 2, 3, 4 \}$. 

    \begin{center}
        \begin{tabular}{c | c c c c c}
            $X$ & 0 & 1 & 2 & 3 & 4 \\
            \hline
            $P(X = x)$ & $\frac{2}{4}$ & $\frac{k+1}{4}$ & $\frac{4k}{4}$ & $\frac{9k-1}{4}$ & $\frac{16k-2}{4}$
        \end{tabular}
    \end{center}
\end{example}

%TODO
PAGE 6 WEEK 4
%TODO

\section{Cumulative Distribution Function}

The probability behaviour of a random variable can be represented in many ways, such as with the probability mass function. Another representation is with the \term{cumulative distribution function}.

\begin{definition}[Cumulative Distribution Function]\index{Cumulative Distribution Function}
    The \term{cumulative distribution function} (CDF) $F(x)$ of a discrete random variable with probability mass function $P(x)$ or $f(x)$ is a function that returns the cumulative (total) probability up to and including $X = x$. $$F(b) = P(X \le b) = \sum_{x \in \{ x \le b \}} P(x)$$ The domain of the CDF is always over the set of real numbers! As such, CDFs are often represented as a piecewise function.
\end{definition}

\begin{example}
    Find the cumulative distribution function for PMF below:

    \begin{center}
        \everymath{\displaystyle}
        \begin{tabular}{ | c | c | c | c | c |}
            \hline
            $x$ & $0$ & $1$ & $2$ & $3$ \\
            \hline
            $P(X = x)$ & $\frac{1}{6}$ & $\frac{1}{2}$ & $\frac{3}{10}$ & $\frac{1}{30}$ \\
            \hline
        \end{tabular}
    \end{center}
    
    $$F(x) = \begin{cases}
        \frac{1}{6} \\
        \frac{2}{3} \\
        \frac{29}{30} \\
        1
    \end{cases}$$
\end{example}

\subsection{Properties of CDF}

\textbf{CDF of a Discrete Random Variable}

For a discrete random variable $X$ with CDF $F(X)$:

\begin{enumerate} \everymath{\displaystyle}
    \item The graph of the CDF will be a \bred{non-decreasing step-function}. That is for $a < b$, $F(a) \le F(b)$. 
    \item The graph of the CDF is \bred{right continuous}. That is, $\lim_{x \to c^+} F(x) = F(c)$. 
    \item $\lim_{x \to \infty} F(x) = 1$
    \item $\lim_{x \to -\infty} F(x) = -1$
\end{enumerate}

%TODO
Add Stuff Here (Week 5 page 17 - 20)
%TODO

\section{Chebyshev's Inequality}

\begin{theorem}[Chebyshev's Inequality]\index{Chebyshev's Inequality}
    Let $X$ be a random variable with mean (expected value) $\mu$ and finite variance $\sigma^2$. Then for any positive $k$, $$P(|x - \mu| {\color{red}~<~} k\sigma) \ge 1 - \frac{1}{k^2}$$
\end{theorem}

\begin{proof}
    By Markov's Inequality: for non negative $x$, $P(x \ge a) \le \frac{E(x)}{a}$, $a > 0$. 

    $\begin{aligned}[t]
        P(|X - \mu| < k\sigma) & = P((x - \mu)^2 < k^2\sigma^2) &\text{since RV's are non-nagative}
    \end{aligned}$

    %TODO
    FINISH THE PROOF
    %TODO
\end{proof}

\begin{example}
    Based on past data, the average daily number of tech support requests at a local call centre is 115 with a standard deviation of 10 calls.

    \begin{enumerate}[label=\alph*)]
        \item What can be said about the fraction of days on which the number of calls received is between 100 and 130?
        
        Dist info: missing 
        
        We are given: $\mu = 115$, $r = 10$

        Let $C$ be the random number of daily calls. 

        $\begin{aligned}[t]
            P(100 \le C \le 130) & = P(-15 \le C - 115 \le 15) \\
                                 & = P(-15 \le C - \mu \le 15) \\
                                 & = P(|C - \mu| \le 15) \\
                                 & = P(|C - \mu| < 16) \\
                                 & = P(|C - \mu| < 1.6\sigma) \\
                                 & \ge 1 - \frac{1}{1.6} = 0.6094
        \end{aligned}$

        $\therefore$ At least $60.94\%$ of the time they will have between 100 to 130 calls a day. 

        \item What number of calls can they expect to receive at least $90\%$ of the time?
    \end{enumerate}
\end{example}

\section{Common Discrete Distributions}

\subsection{Bernoulli Trials}

\begin{definition}[Bernoulli Trials]\index{Bernoulli Trials}
    A \term{Bernoulli trial} is a random experiment consisting of exactly one trial involving two possible outcomes, often called a \itblue{success} or a \itblue{failure}. Let $X$ be the outcome of a Bernoulli trial where 
    %TODO
    FINISH on PAGE 24
    %TODO
\end{definition}

Often, we are interested in modeling the number of successes among multiple trials instead of the results of a single trial:

\begin{definition}[Binomial Distribution]\index{Binomial Distribution}
    A \term{Binomial experiment} consists of $n$ independent and identical Bernoulli trials. The probability of success, $p$, is fixed for each trial. 

    Let $X$ be the random variable representing the number of successes among the $n$ trials. Then $X$ can be modeled by the binomial distribution with parameters $n$ and $p$, denoted as $X \sim \mathrm{Bin}(n, p)$. The binomial distribution has probability mass function: $$O(X = x) = \binom{n}{x} \cdot p^x \cdot (1 - p)^{n - x}$$ If $X \sim \mathrm{Bin}(n, p)$, we can show that $E(X) = np$ and $V(X) = np(1 - p)$
\end{definition}